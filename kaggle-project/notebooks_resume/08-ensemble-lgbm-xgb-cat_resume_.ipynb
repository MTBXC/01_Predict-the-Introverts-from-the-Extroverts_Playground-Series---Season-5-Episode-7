{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91718,"databundleVersionId":12738969,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:54:35.916558Z","iopub.execute_input":"2025-07-14T13:54:35.916862Z","iopub.status.idle":"2025-07-14T13:54:37.089194Z","shell.execute_reply.started":"2025-07-14T13:54:35.916838Z","shell.execute_reply":"2025-07-14T13:54:37.088222Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load Dataset\n\n# Define file paths (Kaggle default path structure)\nTRAIN_PATH = '/kaggle/input/playground-series-s5e7/train.csv'\nTEST_PATH = '/kaggle/input/playground-series-s5e7/test.csv'\n\n# Load the CSV files into DataFrames\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:54:43.582517Z","iopub.execute_input":"2025-07-14T13:54:43.583435Z","iopub.status.idle":"2025-07-14T13:54:43.625695Z","shell.execute_reply.started":"2025-07-14T13:54:43.583381Z","shell.execute_reply":"2025-07-14T13:54:43.624973Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Basic Preprocessing\n\n# Drop the 'id' column as it's not predictive\ntrain_df.drop(columns=['id'], inplace=True)\ntest_df.drop(columns=['id'], inplace=True)\n\n# Method to splits df into numerical and categorical features.\ndef split_numerical_categorical(df):\n    \"\"\"\n    Splits the columns of a DataFrame into numerical and categorical features.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to split.\n\n    Returns:\n    tuple: A tuple containing two lists - numerical columns and categorical columns.\n    \"\"\"\n    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n    categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n    return numerical_cols, categorical_cols\n\n# Separate target variable\ntarget = train_df['Personality']\ntrain_df.drop(columns=['Personality'], inplace=True)\n\n# Split features into numerical and categorical using a custom utility\nnumerical_cols, categorical_cols = split_numerical_categorical(train_df)\n\nprint(\"Numerical features:\", numerical_cols)\nprint(\"Categorical features:\", categorical_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:54:49.261522Z","iopub.execute_input":"2025-07-14T13:54:49.261861Z","iopub.status.idle":"2025-07-14T13:54:49.275382Z","shell.execute_reply.started":"2025-07-14T13:54:49.261834Z","shell.execute_reply":"2025-07-14T13:54:49.274328Z"}},"outputs":[{"name":"stdout","text":"Numerical features: ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\nCategorical features: ['Stage_fear', 'Drained_after_socializing']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Stage 4: Handle Missing Values\n# ==========================================\n# Combine train and test for consistent preprocessing\nfull_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n\n# Impute missing numerical values using Iterative Imputer (e.g., Bayesian Ridge)\nimputer = IterativeImputer(random_state=42)\nfull[numerical_cols] = imputer.fit_transform(full[numerical_cols])\n\n# Imputer for categorical columns with constant value 'Missing'\nimputer_const = SimpleImputer(strategy='constant', fill_value='Missing')\n\n# Apply imputation and preserve column names\nfull_df[categorical_cols] = pd.DataFrame(\n    imputer_const.fit_transform(full_df[categorical_cols]),\n    columns=categorical_cols,\n    index=full_df.index\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:55:09.819230Z","iopub.execute_input":"2025-07-14T13:55:09.819919Z","iopub.status.idle":"2025-07-14T13:55:09.882672Z","shell.execute_reply.started":"2025-07-14T13:55:09.819876Z","shell.execute_reply":"2025-07-14T13:55:09.881784Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Encode Categorical Features\n\n# Apply One-Hot Encoding to categorical features\nfull_encoded = pd.get_dummies(full_df, columns=categorical_cols)\n\n# Split the combined data back into training and test sets\nX_train = full_encoded.iloc[:len(train_df)]\nX_test = full_encoded.iloc[len(train_df):]\ny_train = target.map({'Extrovert': 0, 'Introvert': 1})  # Encode target as binary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:55:15.511446Z","iopub.execute_input":"2025-07-14T13:55:15.511790Z","iopub.status.idle":"2025-07-14T13:55:15.528767Z","shell.execute_reply.started":"2025-07-14T13:55:15.511764Z","shell.execute_reply":"2025-07-14T13:55:15.527966Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#LightGBM\n\n%pip install optuna  \n\n\n# 1. Imports\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport optuna\nimport lightgbm as lgb\n\n\n# 2. Optuna - Hyperparameter Optimization for LightGBM\ndef objective_lgbm(trial):\n    # Define the hyperparameter search space for LightGBM\n    param_lgbm = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 1200),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'num_leaves': trial.suggest_int('num_leaves', 15, 100),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n        'random_state': 42,\n        'n_jobs': -1\n    }\n    \n    # Stratified K-Fold to preserve label distribution across folds\n    skf_lgbm = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    scores_lgbm = []\n\n    for train_idx, val_idx in skf_lgbm.split(X_train, y_train):\n        # Split training and validation data for each fold\n        X_tr_lgbm, X_val_lgbm = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_tr_lgbm, y_val_lgbm = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        \n        # Initialize and train LightGBM model with current parameters\n        model_lgbm = lgb.LGBMClassifier(**param_lgbm, verbose = -1)\n        model_lgbm.fit(\n            X_tr_lgbm, y_tr_lgbm,\n            eval_set=[(X_val_lgbm, y_val_lgbm)],\n            callbacks=[lgb.early_stopping(50, verbose=False)]\n        )\n        \n        # Predict on validation set and compute accuracy\n        val_pred_lgbm = model_lgbm.predict(X_val_lgbm)\n        score_lgbm = accuracy_score(y_val_lgbm, val_pred_lgbm)\n        scores_lgbm.append(score_lgbm)\n    \n    # Return mean cross-validation score\n    return np.mean(scores_lgbm)\n\n# Run Optuna study with the defined objective\nstudy_lgbm = optuna.create_study(direction='maximize')\nstudy_lgbm.optimize(objective_lgbm, n_trials=30)  # Increase n_trials for better optimization\n\n# Display best hyperparameters found by Optuna\nprint(\"Best hyperparameters found by Optuna:\", study_lgbm.best_params)\n\n# 3. Train Final Model with Out-of-Fold (OOF) and Test Predictions\nbest_params_lgbm = study_lgbm.best_params\nbest_params_lgbm.update({'random_state': 42, 'n_jobs': -1})\n\n# Use 10-fold Stratified CV for final training and predictions\nskf_lgbm = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nscores_lgbm = []\n\n# Initialize arrays to store predictions\ntest_preds_lgbm = np.zeros((len(X_test), skf_lgbm.n_splits))  # Test predictions per fold\noof_preds_lgbm = np.zeros(len(X_train))                       # Out-of-fold predictions\n\nfor fold, (train_idx, val_idx) in enumerate(skf_lgbm.split(X_train, y_train)):\n    # Split data into train and validation sets\n    X_tr_lgbm, X_val_lgbm = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr_lgbm, y_val_lgbm = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # Train model with best parameters from Optuna\n    model_lgbm = lgb.LGBMClassifier(**best_params_lgbm)\n    model_lgbm.fit(\n        X_tr_lgbm, y_tr_lgbm,\n        eval_set=[(X_val_lgbm, y_val_lgbm)],\n        callbacks=[lgb.early_stopping(50, verbose=False)]\n    )\n    \n    # Predict and evaluate on validation set\n    val_pred_lgbm = model_lgbm.predict(X_val_lgbm)\n    score_lgbm = accuracy_score(y_val_lgbm, val_pred_lgbm)\n    scores_lgbm.append(score_lgbm)\n    \n    # Store OOF predicted probabilities for ensemble/blending\n    oof_preds_lgbm[val_idx] = model_lgbm.predict_proba(X_val_lgbm)[:, 1]\n    \n    # Store predictions on test set for this fold\n    test_preds_lgbm[:, fold] = model_lgbm.predict_proba(X_test)[:, 1]\n\n# Convert OOF probabilities to binary predictions using 0.5 threshold\noof_binary_lgbm = (oof_preds_lgbm > 0.5).astype(int)\nprint(\"OOF accuracy:\", accuracy_score(y_train, oof_binary_lgbm))\n\n# Average test predictions across all folds\nmean_preds_lgbm = test_preds_lgbm.mean(axis=1)\nfinal_test_pred_lgbm = (mean_preds_lgbm > 0.5).astype(int)\n\n# Print individual fold accuracies and overall mean CV score\nprint(f'Fold accuracies: {scores_lgbm}')\nprint(f'Mean CV accuracy: {np.mean(scores_lgbm):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:55:22.881765Z","iopub.execute_input":"2025-07-14T13:55:22.882111Z","iopub.status.idle":"2025-07-14T13:56:04.413869Z","shell.execute_reply.started":"2025-07-14T13:55:22.882086Z","shell.execute_reply":"2025-07-14T13:56:04.412743Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-07-14 13:55:28,508] A new study created in memory with name: no-name-16c62d3f-1664-493f-a0bf-403a905b5542\n[I 2025-07-14 13:55:29,944] Trial 0 finished with value: 0.9690672373740465 and parameters: {'n_estimators': 435, 'learning_rate': 0.04352250174678393, 'max_depth': 11, 'num_leaves': 31, 'subsample': 0.6608003265215836, 'colsample_bytree': 0.8181756761107688, 'reg_alpha': 3.896095902964353, 'reg_lambda': 4.151914769252441}. Best is trial 0 with value: 0.9690672373740465.\n[I 2025-07-14 13:55:30,894] Trial 1 finished with value: 0.9691751995872719 and parameters: {'n_estimators': 678, 'learning_rate': 0.09695108833761601, 'max_depth': 10, 'num_leaves': 98, 'subsample': 0.6554105109545882, 'colsample_bytree': 0.6406946657624135, 'reg_alpha': 4.469846168998027, 'reg_lambda': 4.305905465124664}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:31,484] Trial 2 finished with value: 0.9690132854148995 and parameters: {'n_estimators': 1161, 'learning_rate': 0.14234751693737438, 'max_depth': 3, 'num_leaves': 20, 'subsample': 0.8689340544531495, 'colsample_bytree': 0.8304788069996425, 'reg_alpha': 4.080241482429786, 'reg_lambda': 0.7131321631683601}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:32,578] Trial 3 finished with value: 0.9689592897345541 and parameters: {'n_estimators': 400, 'learning_rate': 0.0853772036782457, 'max_depth': 7, 'num_leaves': 65, 'subsample': 0.6763479977026662, 'colsample_bytree': 0.9534501570106387, 'reg_alpha': 1.792026402800887, 'reg_lambda': 4.951314933916658}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:33,264] Trial 4 finished with value: 0.9690672519477793 and parameters: {'n_estimators': 596, 'learning_rate': 0.13638349234963904, 'max_depth': 7, 'num_leaves': 55, 'subsample': 0.7573215175233897, 'colsample_bytree': 0.6929649281792848, 'reg_alpha': 4.182157249133247, 'reg_lambda': 0.5931775066849682}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:33,749] Trial 5 finished with value: 0.9690132854148995 and parameters: {'n_estimators': 260, 'learning_rate': 0.26417825795441724, 'max_depth': 11, 'num_leaves': 67, 'subsample': 0.9202822872106051, 'colsample_bytree': 0.7721205825852899, 'reg_alpha': 3.7530750508382456, 'reg_lambda': 0.9341724453960204}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:35,552] Trial 6 finished with value: 0.9690132999886323 and parameters: {'n_estimators': 342, 'learning_rate': 0.05310183785241047, 'max_depth': 12, 'num_leaves': 96, 'subsample': 0.7128481633395233, 'colsample_bytree': 0.9874661057573906, 'reg_alpha': 1.2560139304011368, 'reg_lambda': 0.2201933457584504}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:36,464] Trial 7 finished with value: 0.969013256267434 and parameters: {'n_estimators': 698, 'learning_rate': 0.08320710626346468, 'max_depth': 11, 'num_leaves': 22, 'subsample': 0.8890368238706642, 'colsample_bytree': 0.9676207627406541, 'reg_alpha': 3.4705312541461004, 'reg_lambda': 2.04624917641508}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:36,947] Trial 8 finished with value: 0.9690132854148995 and parameters: {'n_estimators': 298, 'learning_rate': 0.16498189923048193, 'max_depth': 3, 'num_leaves': 79, 'subsample': 0.6695788402027647, 'colsample_bytree': 0.942164027110599, 'reg_alpha': 2.8080833289861626, 'reg_lambda': 2.555102620871276}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:39,443] Trial 9 finished with value: 0.9691212330543921 and parameters: {'n_estimators': 1035, 'learning_rate': 0.019544177969834663, 'max_depth': 6, 'num_leaves': 96, 'subsample': 0.8262139848266865, 'colsample_bytree': 0.6656770381332452, 'reg_alpha': 1.6600659922352885, 'reg_lambda': 1.7166188574796841}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:41,979] Trial 10 finished with value: 0.9691212330543921 and parameters: {'n_estimators': 878, 'learning_rate': 0.2218072783035214, 'max_depth': 9, 'num_leaves': 43, 'subsample': 0.9857160574967956, 'colsample_bytree': 0.6084255310933364, 'reg_alpha': 0.20807813362317296, 'reg_lambda': 3.4331462504769124}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:45,504] Trial 11 finished with value: 0.969013241693701 and parameters: {'n_estimators': 947, 'learning_rate': 0.012762779606923957, 'max_depth': 5, 'num_leaves': 99, 'subsample': 0.7952368263206491, 'colsample_bytree': 0.6269321915229475, 'reg_alpha': 4.950396658575287, 'reg_lambda': 2.0145173755123658}. Best is trial 1 with value: 0.9691751995872719.\n[I 2025-07-14 13:55:46,585] Trial 12 finished with value: 0.9691752141610047 and parameters: {'n_estimators': 954, 'learning_rate': 0.10094933619545648, 'max_depth': 9, 'num_leaves': 85, 'subsample': 0.6094284699747292, 'colsample_bytree': 0.6926710694677003, 'reg_alpha': 2.006077147745537, 'reg_lambda': 3.12991185471698}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:47,606] Trial 13 finished with value: 0.9691212184806591 and parameters: {'n_estimators': 790, 'learning_rate': 0.10091773988528877, 'max_depth': 9, 'num_leaves': 80, 'subsample': 0.603691094731339, 'colsample_bytree': 0.743584225773171, 'reg_alpha': 2.5790598768581257, 'reg_lambda': 3.4146153295552075}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:49,063] Trial 14 finished with value: 0.9690672519477795 and parameters: {'n_estimators': 608, 'learning_rate': 0.17646162614650435, 'max_depth': 9, 'num_leaves': 79, 'subsample': 0.6033470957311723, 'colsample_bytree': 0.7129465763733386, 'reg_alpha': 0.7144371198937678, 'reg_lambda': 4.990654107834263}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:50,033] Trial 15 finished with value: 0.9690672373740465 and parameters: {'n_estimators': 1180, 'learning_rate': 0.10968654128629293, 'max_depth': 9, 'num_leaves': 88, 'subsample': 0.736197701351266, 'colsample_bytree': 0.6603700168714626, 'reg_alpha': 2.972007983034046, 'reg_lambda': 3.702680127070134}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:50,628] Trial 16 finished with value: 0.9691752141610047 and parameters: {'n_estimators': 781, 'learning_rate': 0.2116821290868816, 'max_depth': 10, 'num_leaves': 85, 'subsample': 0.6364897991601087, 'colsample_bytree': 0.7381736087783546, 'reg_alpha': 4.976882548476766, 'reg_lambda': 2.846006403345143}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:51,567] Trial 17 finished with value: 0.9691212476281251 and parameters: {'n_estimators': 999, 'learning_rate': 0.2144024840162903, 'max_depth': 8, 'num_leaves': 68, 'subsample': 0.6088011522036615, 'colsample_bytree': 0.8876480744068914, 'reg_alpha': 2.058965974179839, 'reg_lambda': 2.808715480579367}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:52,227] Trial 18 finished with value: 0.9691212330543921 and parameters: {'n_estimators': 829, 'learning_rate': 0.20182533147629889, 'max_depth': 12, 'num_leaves': 52, 'subsample': 0.7070884907860756, 'colsample_bytree': 0.772927006993784, 'reg_alpha': 3.318888406024572, 'reg_lambda': 3.0953133077442705}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:52,894] Trial 19 finished with value: 0.9689592605870884 and parameters: {'n_estimators': 1077, 'learning_rate': 0.2983433358137083, 'max_depth': 10, 'num_leaves': 86, 'subsample': 0.7758698847583297, 'colsample_bytree': 0.8602826086781686, 'reg_alpha': 2.1912459325193256, 'reg_lambda': 1.5843083887766456}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:53,369] Trial 20 finished with value: 0.9690132708411667 and parameters: {'n_estimators': 500, 'learning_rate': 0.24711035048220492, 'max_depth': 5, 'num_leaves': 72, 'subsample': 0.636709566940918, 'colsample_bytree': 0.7174955932684492, 'reg_alpha': 4.899095214413619, 'reg_lambda': 4.074582045923086}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:54,152] Trial 21 finished with value: 0.9691212039069264 and parameters: {'n_estimators': 734, 'learning_rate': 0.12464177467775055, 'max_depth': 10, 'num_leaves': 89, 'subsample': 0.6413600287392187, 'colsample_bytree': 0.6589340451971125, 'reg_alpha': 4.594748029115719, 'reg_lambda': 4.366841952698808}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:54,766] Trial 22 finished with value: 0.9689592751608211 and parameters: {'n_estimators': 636, 'learning_rate': 0.1816523443338954, 'max_depth': 10, 'num_leaves': 100, 'subsample': 0.6990740035179361, 'colsample_bytree': 0.7482076147541287, 'reg_alpha': 4.456548802984207, 'reg_lambda': 2.9855252035640683}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:56,096] Trial 23 finished with value: 0.9690672373740465 and parameters: {'n_estimators': 884, 'learning_rate': 0.06561934651168976, 'max_depth': 8, 'num_leaves': 90, 'subsample': 0.6423139571901487, 'colsample_bytree': 0.6380836691279573, 'reg_alpha': 1.296159847343077, 'reg_lambda': 3.781906555624996}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:56,760] Trial 24 finished with value: 0.9690132562674337 and parameters: {'n_estimators': 721, 'learning_rate': 0.1543239234058716, 'max_depth': 10, 'num_leaves': 82, 'subsample': 0.6870878502928736, 'colsample_bytree': 0.6899715264191478, 'reg_alpha': 4.517356324281259, 'reg_lambda': 2.4064694431277}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:57,665] Trial 25 finished with value: 0.9690672373740465 and parameters: {'n_estimators': 538, 'learning_rate': 0.11184994235256848, 'max_depth': 8, 'num_leaves': 74, 'subsample': 0.735327802128768, 'colsample_bytree': 0.6022272125873808, 'reg_alpha': 3.066718423063084, 'reg_lambda': 4.623522543233321}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:58,846] Trial 26 finished with value: 0.969013256267434 and parameters: {'n_estimators': 930, 'learning_rate': 0.0808638261133581, 'max_depth': 11, 'num_leaves': 61, 'subsample': 0.6323888599791969, 'colsample_bytree': 0.7297715764946998, 'reg_alpha': 2.4235193361431357, 'reg_lambda': 3.339715397553021}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:59,341] Trial 27 finished with value: 0.9691212039069264 and parameters: {'n_estimators': 779, 'learning_rate': 0.26085848924730765, 'max_depth': 9, 'num_leaves': 93, 'subsample': 0.6567061055563722, 'colsample_bytree': 0.7878506414831566, 'reg_alpha': 4.949641261259279, 'reg_lambda': 2.372978033877179}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:55:59,964] Trial 28 finished with value: 0.9690132562674337 and parameters: {'n_estimators': 651, 'learning_rate': 0.18732631394506372, 'max_depth': 12, 'num_leaves': 84, 'subsample': 0.8349189767934091, 'colsample_bytree': 0.69400796822182, 'reg_alpha': 3.507518404619628, 'reg_lambda': 1.3428399161284537}. Best is trial 12 with value: 0.9691752141610047.\n[I 2025-07-14 13:56:02,076] Trial 29 finished with value: 0.969013241693701 and parameters: {'n_estimators': 1102, 'learning_rate': 0.029765700778470322, 'max_depth': 10, 'num_leaves': 44, 'subsample': 0.6214284790249931, 'colsample_bytree': 0.8061221334687128, 'reg_alpha': 3.857419352667449, 'reg_lambda': 4.130477116739093}. Best is trial 12 with value: 0.9691752141610047.\n","output_type":"stream"},{"name":"stdout","text":"Best hyperparameters found by Optuna: {'n_estimators': 954, 'learning_rate': 0.10094933619545648, 'max_depth': 9, 'num_leaves': 85, 'subsample': 0.6094284699747292, 'colsample_bytree': 0.6926710694677003, 'reg_alpha': 2.006077147745537, 'reg_lambda': 3.12991185471698}\nOOF accuracy: 0.969067156121788\nFold accuracies: [0.9681597409606044, 0.9719373988127361, 0.9724770642201835, 0.963302752293578, 0.9654427645788337, 0.9670626349892009, 0.9703023758099352, 0.9697624190064795, 0.9708423326133909, 0.9713822894168467]\nMean CV accuracy: 0.9691\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#catboost\n\n%pip install catboost \n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom catboost import CatBoostClassifier\n\ncat_features_cat = []  # <- Set to column indices/names if you have categorical features\n\n# 1. Optuna - Hyperparameter Optimization for CatBoost\ndef objective_cat(trial):\n    # Define the hyperparameter search space\n    param_cat = {\n        'iterations': trial.suggest_int('iterations', 200, 1200),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'depth': trial.suggest_int('depth', 3, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n        'random_strength': trial.suggest_float('random_strength', 0.1, 2.0),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'random_seed': 42,\n        'verbose': 0,  # Suppress CatBoost training output\n        'loss_function': 'Logloss',\n        'eval_metric': 'Accuracy',\n        'cat_features': cat_features_cat\n    }\n\n    # Perform 5-fold stratified cross-validation\n    skf_cat = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    scores_cat = []\n\n    for train_idx, val_idx in skf_cat.split(X_train, y_train):\n        # Split data into training and validation sets\n        X_tr_cat, X_val_cat = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_tr_cat, y_val_cat = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        \n        # Train the CatBoost model with the current hyperparameters\n        model_cat = CatBoostClassifier(**param_cat)\n        model_cat.fit(\n            X_tr_cat, y_tr_cat,\n            eval_set=(X_val_cat, y_val_cat),\n            early_stopping_rounds=50,\n            use_best_model=True\n        )\n        \n        # Evaluate model on validation data\n        val_pred_cat = model_cat.predict(X_val_cat)\n        score_cat = accuracy_score(y_val_cat, val_pred_cat)\n        scores_cat.append(score_cat)\n    \n    # Return the average validation score across folds\n    return np.mean(scores_cat)\n\n# Run the Optuna study to maximize the validation accuracy\nstudy_cat = optuna.create_study(direction='maximize')\nstudy_cat.optimize(objective_cat, n_trials=30)  # Increase n_trials for more thorough optimization\n\n# Print the best hyperparameters found by Optuna\nprint(\"Best hyperparameters found by Optuna:\", study_cat.best_params)\n\n# 2. Train Final Model Using Out-of-Fold (OOF) and Test Predictions\n\n# Prepare best parameters and update with fixed values\nbest_params_cat = study_cat.best_params\nbest_params_cat.update({\n    'random_seed': 42,\n    'verbose': 0,\n    'loss_function': 'Logloss',\n    'eval_metric': 'Accuracy',\n    'cat_features': cat_features_cat\n})\n\n# Use 10-fold stratified cross-validation for final model evaluation\nskf_cat = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nscores_cat = []\n\n# Arrays to store predictions\ntest_preds_cat = np.zeros((len(X_test), skf_cat.n_splits))  # Predictions for each test fold\noof_preds_cat = np.zeros(len(X_train))                       # Out-of-Fold predictions for training data\n\nfor fold, (train_idx, val_idx) in enumerate(skf_cat.split(X_train, y_train)):\n    # Split data into training and validation sets\n    X_tr_cat, X_val_cat = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr_cat, y_val_cat = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # Train model using best parameters\n    model_cat = CatBoostClassifier(**best_params_cat)\n    model_cat.fit(\n        X_tr_cat, y_tr_cat,\n        eval_set=(X_val_cat, y_val_cat),\n        early_stopping_rounds=50,\n        use_best_model=True\n    )\n    \n    # Validation accuracy\n    val_pred_cat = model_cat.predict(X_val_cat)\n    score_cat = accuracy_score(y_val_cat, val_pred_cat)\n    scores_cat.append(score_cat)\n    \n    # Store Out-of-Fold predicted probabilities\n    oof_preds_cat[val_idx] = model_cat.predict_proba(X_val_cat)[:, 1]\n    \n    # Store predicted probabilities for the test set (per fold)\n    test_preds_cat[:, fold] = model_cat.predict_proba(X_test)[:, 1]\n\n# Convert OOF probabilities into binary predictions using 0.5 threshold\noof_binary_cat = (oof_preds_cat > 0.5).astype(int)\nprint(\"OOF accuracy:\", accuracy_score(y_train, oof_binary_cat))\n\n# Average the test predictions across all folds\nmean_preds_cat = test_preds_cat.mean(axis=1)\nfinal_test_pred_cat = (mean_preds_cat > 0.5).astype(int)\n\n# Print fold-wise and mean cross-validation accuracy\nprint(f'Fold accuracy: {scores_cat}')\nprint(f'Mean CV accuracy: {np.mean(scores_cat):.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:56:06.740268Z","iopub.execute_input":"2025-07-14T13:56:06.741257Z","iopub.status.idle":"2025-07-14T13:57:22.922527Z","shell.execute_reply.started":"2025-07-14T13:56:06.741227Z","shell.execute_reply":"2025-07-14T13:57:22.921344Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\nRequirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\nRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.16.0->catboost) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.16.0->catboost) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.16.0->catboost) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.16.0->catboost) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.16.0->catboost) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.16.0->catboost) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-07-14 13:56:10,606] A new study created in memory with name: no-name-9b56260a-fe05-459d-bf52-d5a75c8732f0\n[I 2025-07-14 13:56:13,245] Trial 0 finished with value: 0.9693371720545757 and parameters: {'iterations': 686, 'learning_rate': 0.035187979534323004, 'depth': 6, 'l2_leaf_reg': 7.966402569123501, 'random_strength': 1.375756129128048, 'bagging_temperature': 0.4131998134652384, 'border_count': 94}. Best is trial 0 with value: 0.9693371720545757.\n[I 2025-07-14 13:56:16,262] Trial 1 finished with value: 0.9692831763742301 and parameters: {'iterations': 611, 'learning_rate': 0.17795485558620336, 'depth': 8, 'l2_leaf_reg': 8.097241660798161, 'random_strength': 1.486650125276139, 'bagging_temperature': 0.18067190556629353, 'border_count': 103}. Best is trial 0 with value: 0.9693371720545757.\n[I 2025-07-14 13:56:20,055] Trial 2 finished with value: 0.9692291952676175 and parameters: {'iterations': 660, 'learning_rate': 0.0796723756480526, 'depth': 9, 'l2_leaf_reg': 2.6292567678488403, 'random_strength': 1.199161313432347, 'bagging_temperature': 0.7934808808693945, 'border_count': 234}. Best is trial 0 with value: 0.9693371720545757.\n[I 2025-07-14 13:56:25,491] Trial 3 finished with value: 0.9692291952676175 and parameters: {'iterations': 576, 'learning_rate': 0.20910330574619398, 'depth': 10, 'l2_leaf_reg': 7.113541990587115, 'random_strength': 1.8864062296308546, 'bagging_temperature': 0.29840490604213366, 'border_count': 99}. Best is trial 0 with value: 0.9693371720545757.\n[I 2025-07-14 13:56:26,852] Trial 4 finished with value: 0.9690672519477793 and parameters: {'iterations': 672, 'learning_rate': 0.26247555669197814, 'depth': 3, 'l2_leaf_reg': 7.810971895634853, 'random_strength': 0.5979262722456443, 'bagging_temperature': 0.8839952333389456, 'border_count': 217}. Best is trial 0 with value: 0.9693371720545757.\n[I 2025-07-14 13:56:28,825] Trial 5 finished with value: 0.9692832055216961 and parameters: {'iterations': 525, 'learning_rate': 0.1879057799288397, 'depth': 6, 'l2_leaf_reg': 2.784916228945977, 'random_strength': 1.8044911630080385, 'bagging_temperature': 0.8911722016019946, 'border_count': 207}. Best is trial 0 with value: 0.9693371720545757.\n[I 2025-07-14 13:56:35,067] Trial 6 finished with value: 0.9694990862269479 and parameters: {'iterations': 293, 'learning_rate': 0.29120425758913976, 'depth': 10, 'l2_leaf_reg': 9.180519457373027, 'random_strength': 1.5988390440417675, 'bagging_temperature': 0.527338451047905, 'border_count': 198}. Best is trial 6 with value: 0.9694990862269479.\n[I 2025-07-14 13:56:36,996] Trial 7 finished with value: 0.9694991445218795 and parameters: {'iterations': 500, 'learning_rate': 0.24409222313426188, 'depth': 6, 'l2_leaf_reg': 2.783962694810973, 'random_strength': 0.21941475552373815, 'bagging_temperature': 0.25585584420172347, 'border_count': 88}. Best is trial 7 with value: 0.9694991445218795.\n[I 2025-07-14 13:56:38,736] Trial 8 finished with value: 0.9693911677349213 and parameters: {'iterations': 636, 'learning_rate': 0.18088626641503924, 'depth': 5, 'l2_leaf_reg': 1.1765102837973238, 'random_strength': 1.8593283483158263, 'bagging_temperature': 0.3645968828582219, 'border_count': 80}. Best is trial 7 with value: 0.9694991445218795.\n[I 2025-07-14 13:56:40,282] Trial 9 finished with value: 0.9692831763742303 and parameters: {'iterations': 392, 'learning_rate': 0.07796946506796876, 'depth': 3, 'l2_leaf_reg': 4.651703349060665, 'random_strength': 0.40073567348888006, 'bagging_temperature': 0.18188321324695633, 'border_count': 50}. Best is trial 7 with value: 0.9694991445218795.\n[I 2025-07-14 13:56:42,991] Trial 10 finished with value: 0.9694991299481467 and parameters: {'iterations': 988, 'learning_rate': 0.2458007140322902, 'depth': 8, 'l2_leaf_reg': 4.943861338364165, 'random_strength': 0.10225683521720032, 'bagging_temperature': 0.05478525985388194, 'border_count': 160}. Best is trial 7 with value: 0.9694991445218795.\n[I 2025-07-14 13:56:45,747] Trial 11 finished with value: 0.9694451196940683 and parameters: {'iterations': 1014, 'learning_rate': 0.23181416354787568, 'depth': 8, 'l2_leaf_reg': 5.152610930012537, 'random_strength': 0.11624653328426177, 'bagging_temperature': 1.4777610126831542e-05, 'border_count': 161}. Best is trial 7 with value: 0.9694991445218795.\n[I 2025-07-14 13:56:48,095] Trial 12 finished with value: 0.9694451342678011 and parameters: {'iterations': 964, 'learning_rate': 0.12816185181396125, 'depth': 7, 'l2_leaf_reg': 3.5185459672806596, 'random_strength': 0.7589620183213563, 'bagging_temperature': 0.005489162482421303, 'border_count': 148}. Best is trial 7 with value: 0.9694991445218795.\n[I 2025-07-14 13:56:49,920] Trial 13 finished with value: 0.9696071067351048 and parameters: {'iterations': 1165, 'learning_rate': 0.25373372382893394, 'depth': 5, 'l2_leaf_reg': 5.871701163597772, 'random_strength': 0.13833477638548233, 'bagging_temperature': 0.5810786053414668, 'border_count': 129}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:56:51,925] Trial 14 finished with value: 0.9695530964810264 and parameters: {'iterations': 1188, 'learning_rate': 0.29632369060334113, 'depth': 5, 'l2_leaf_reg': 6.271073073399691, 'random_strength': 0.7937155711522211, 'bagging_temperature': 0.6091032958565339, 'border_count': 35}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:56:54,031] Trial 15 finished with value: 0.9692831909479629 and parameters: {'iterations': 1199, 'learning_rate': 0.28617045151639703, 'depth': 4, 'l2_leaf_reg': 6.34265597351971, 'random_strength': 0.8698387248319157, 'bagging_temperature': 0.6450662484686134, 'border_count': 33}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:56:55,933] Trial 16 finished with value: 0.9694991153744137 and parameters: {'iterations': 1185, 'learning_rate': 0.2960943233160045, 'depth': 5, 'l2_leaf_reg': 6.360975312546754, 'random_strength': 1.0448398000911723, 'bagging_temperature': 0.6613169243169605, 'border_count': 133}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:56:57,410] Trial 17 finished with value: 0.9692292098413503 and parameters: {'iterations': 815, 'learning_rate': 0.12749832669679814, 'depth': 4, 'l2_leaf_reg': 9.474984953220531, 'random_strength': 0.37128929873734473, 'bagging_temperature': 0.560018644556024, 'border_count': 58}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:56:59,014] Trial 18 finished with value: 0.9694991299481466 and parameters: {'iterations': 839, 'learning_rate': 0.2662505445909012, 'depth': 5, 'l2_leaf_reg': 6.059456374508323, 'random_strength': 0.6038950367382514, 'bagging_temperature': 0.7433283443370353, 'border_count': 123}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:57:00,492] Trial 19 finished with value: 0.9693371866283087 and parameters: {'iterations': 1089, 'learning_rate': 0.22093021339149646, 'depth': 4, 'l2_leaf_reg': 4.089053506527216, 'random_strength': 0.4489030741579705, 'bagging_temperature': 0.985852615725388, 'border_count': 181}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:57:02,650] Trial 20 finished with value: 0.9693371574808429 and parameters: {'iterations': 810, 'learning_rate': 0.14971666498641284, 'depth': 7, 'l2_leaf_reg': 7.07622446761013, 'random_strength': 0.9516553573258593, 'bagging_temperature': 0.4583727004087306, 'border_count': 248}. Best is trial 13 with value: 0.9696071067351048.\n[I 2025-07-14 13:57:04,765] Trial 21 finished with value: 0.9696610586942519 and parameters: {'iterations': 454, 'learning_rate': 0.24988917004960992, 'depth': 6, 'l2_leaf_reg': 1.4695424781821629, 'random_strength': 0.2784429275508974, 'bagging_temperature': 0.6094263996171422, 'border_count': 69}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:06,636] Trial 22 finished with value: 0.9696071067351049 and parameters: {'iterations': 261, 'learning_rate': 0.2667088193120912, 'depth': 5, 'l2_leaf_reg': 5.570675689893595, 'random_strength': 0.6326333158675836, 'bagging_temperature': 0.7117643572375166, 'border_count': 68}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:08,405] Trial 23 finished with value: 0.9694451488415339 and parameters: {'iterations': 202, 'learning_rate': 0.20434700910153142, 'depth': 6, 'l2_leaf_reg': 1.353791023958653, 'random_strength': 0.3356295542085891, 'bagging_temperature': 0.7056029428532213, 'border_count': 72}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:10,287] Trial 24 finished with value: 0.9692291952676175 and parameters: {'iterations': 405, 'learning_rate': 0.26652790056153164, 'depth': 7, 'l2_leaf_reg': 5.58403539545558, 'random_strength': 0.5786289519178496, 'bagging_temperature': 0.799986337145477, 'border_count': 119}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:11,721] Trial 25 finished with value: 0.9693911385874555 and parameters: {'iterations': 357, 'learning_rate': 0.250055487600509, 'depth': 4, 'l2_leaf_reg': 4.071847959338759, 'random_strength': 0.2677489415842797, 'bagging_temperature': 0.5665569755052301, 'border_count': 59}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:13,285] Trial 26 finished with value: 0.9694451634152669 and parameters: {'iterations': 213, 'learning_rate': 0.22034606857609468, 'depth': 5, 'l2_leaf_reg': 2.2084394190248013, 'random_strength': 0.540170643501259, 'bagging_temperature': 0.48381014708791725, 'border_count': 112}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:15,015] Trial 27 finished with value: 0.9693911968823871 and parameters: {'iterations': 462, 'learning_rate': 0.2680324094342912, 'depth': 6, 'l2_leaf_reg': 4.199994643304954, 'random_strength': 0.7059188152536904, 'bagging_temperature': 0.7166813004325218, 'border_count': 72}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:16,671] Trial 28 finished with value: 0.9693911531611885 and parameters: {'iterations': 320, 'learning_rate': 0.15613071121018685, 'depth': 4, 'l2_leaf_reg': 5.548133389591868, 'random_strength': 0.18636864271486353, 'bagging_temperature': 0.8617500235697104, 'border_count': 141}. Best is trial 21 with value: 0.9696610586942519.\n[I 2025-07-14 13:57:19,514] Trial 29 finished with value: 0.9692292098413503 and parameters: {'iterations': 267, 'learning_rate': 0.05449502581379209, 'depth': 6, 'l2_leaf_reg': 6.887182436900391, 'random_strength': 1.215128830480662, 'bagging_temperature': 0.3550042288327429, 'border_count': 88}. Best is trial 21 with value: 0.9696610586942519.\n","output_type":"stream"},{"name":"stdout","text":"Best hyperparameters found by Optuna: {'iterations': 454, 'learning_rate': 0.24988917004960992, 'depth': 6, 'l2_leaf_reg': 1.4695424781821629, 'random_strength': 0.2784429275508974, 'bagging_temperature': 0.6094263996171422, 'border_count': 69}\nOOF accuracy: 0.9697689483912761\nFold accuracy: [0.9681597409606044, 0.9730167296276309, 0.9724770642201835, 0.9638424177010254, 0.9665226781857451, 0.9676025917926566, 0.9724622030237581, 0.9708423326133909, 0.9708423326133909, 0.9719222462203023]\nMean CV accuracy: 0.9698\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import xgboost as xgb\n\n# 1. Optuna - hyperparameter optimization\ndef objective_xgb(trial):\n    # Define the hyperparameter search space\n    param_xgb = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 1200),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n        'random_state': 42,\n        'n_jobs': -1,\n        'use_label_encoder': False,\n        'eval_metric': 'logloss',\n        'early_stopping_rounds': 50\n    }\n\n    # 5-fold Stratified Cross-Validation\n    skf_xgb = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    scores_xgb = []\n\n    for train_idx, val_idx in skf_xgb.split(X_train, y_train):\n        # Split training and validation data\n        X_tr_xgb, X_val_xgb = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_tr_xgb, y_val_xgb = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n        # Train the XGBoost model with current hyperparameters\n        model_xgb = xgb.XGBClassifier(**param_xgb)\n        model_xgb.fit(\n            X_tr_xgb, y_tr_xgb,\n            eval_set=[(X_val_xgb, y_val_xgb)],\n            verbose=False  # Suppress output\n        )\n\n        # Evaluate accuracy and store the result\n        scores_xgb.append(accuracy_score(y_val_xgb, model_xgb.predict(X_val_xgb)))\n\n    # Return mean cross-validation score\n    return np.mean(scores_xgb)\n\n# Run Optuna optimization process\nstudy_xgb = optuna.create_study(direction='maximize')\nstudy_xgb.optimize(objective_xgb, n_trials=30)\n\n# Prepare final best parameters for training\nbest_params_xgb = study_xgb.best_params\nbest_params_xgb.update({\n    'random_state': 42,\n    'n_jobs': -1,\n    'use_label_encoder': False,\n    'eval_metric': 'logloss',\n    'early_stopping_rounds': 50\n})\n\n# 10-fold Stratified Cross-Validation for final training and evaluation\nskf_xgb = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nscores_xgb = []\n\n# Arrays to hold predictions\ntest_preds_xgb = np.zeros((len(X_test), skf_xgb.n_splits))  # Test set predictions\noof_preds_xgb = np.zeros(len(X_train))                      # Out-of-fold predictions\n\nfor fold, (train_idx, val_idx) in enumerate(skf_xgb.split(X_train, y_train)):\n    # Split into training and validation sets for this fold\n    X_tr_xgb, X_val_xgb = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr_xgb, y_val_xgb = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    # Train the model using best parameters\n    model_xgb = xgb.XGBClassifier(**best_params_xgb)\n    model_xgb.fit(\n        X_tr_xgb, y_tr_xgb,\n        eval_set=[(X_val_xgb, y_val_xgb)],\n        verbose=False\n    )\n\n    # Save fold accuracy\n    scores_xgb.append(accuracy_score(y_val_xgb, model_xgb.predict(X_val_xgb)))\n\n    # Save OOF predictions (probability for class 1)\n    oof_preds_xgb[val_idx] = model_xgb.predict_proba(X_val_xgb)[:, 1]\n\n    # Save test set predictions for this fold\n    test_preds_xgb[:, fold] = model_xgb.predict_proba(X_test)[:, 1]\n\n# Convert OOF probabilities to binary predictions using 0.5 threshold\noof_binary_xgb = (oof_preds_xgb > 0.5).astype(int)\n\n# Evaluate OOF accuracy\nprint(\"OOF accuracy:\", accuracy_score(y_train, oof_binary_xgb))\n\n# Print accuracy for each fold\nmean_score_xgb = np.mean(scores_xgb)\nprint(\"Fold accuracy:\", scores_xgb)\nprint(f\"Mean CV accuracy: {mean_score_xgb:.4f}\")\n\n# Average predictions across all folds for final test prediction\nmean_preds_xgb = test_preds_xgb.mean(axis=1)\nfinal_test_pred_xgb = (mean_preds_xgb > 0.5).astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:57:42.108395Z","iopub.execute_input":"2025-07-14T13:57:42.109294Z","iopub.status.idle":"2025-07-14T13:58:20.915653Z","shell.execute_reply.started":"2025-07-14T13:57:42.109260Z","shell.execute_reply":"2025-07-14T13:58:20.914703Z"}},"outputs":[{"name":"stderr","text":"[I 2025-07-14 13:57:42,119] A new study created in memory with name: no-name-e34802e4-3033-4080-86b4-4ace2f2c9e88\n[I 2025-07-14 13:57:45,304] Trial 0 finished with value: 0.9691212476281249 and parameters: {'n_estimators': 1138, 'learning_rate': 0.0225390750456084, 'max_depth': 3, 'subsample': 0.7040301477604416, 'colsample_bytree': 0.9225957629789338, 'gamma': 3.881021285530047, 'reg_alpha': 2.2159691455090593, 'reg_lambda': 2.3193656282100372}. Best is trial 0 with value: 0.9691212476281249.\n[I 2025-07-14 13:57:46,373] Trial 1 finished with value: 0.9692831909479629 and parameters: {'n_estimators': 633, 'learning_rate': 0.24046302467173788, 'max_depth': 8, 'subsample': 0.7701103700613223, 'colsample_bytree': 0.675378629373807, 'gamma': 2.898118435138291, 'reg_alpha': 2.07359866773377, 'reg_lambda': 4.362464011622331}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:48,151] Trial 2 finished with value: 0.9689053377754071 and parameters: {'n_estimators': 1038, 'learning_rate': 0.06244594338738301, 'max_depth': 9, 'subsample': 0.804457876172358, 'colsample_bytree': 0.9930063029763323, 'gamma': 0.1738114057061818, 'reg_alpha': 0.6290648120109638, 'reg_lambda': 1.0840228198203916}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:48,940] Trial 3 finished with value: 0.9689052940542086 and parameters: {'n_estimators': 434, 'learning_rate': 0.23077410849617655, 'max_depth': 6, 'subsample': 0.7591345609827829, 'colsample_bytree': 0.8291837355174873, 'gamma': 1.287870117388909, 'reg_alpha': 2.5760992937280047, 'reg_lambda': 3.200849819007848}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:49,838] Trial 4 finished with value: 0.9689053086279413 and parameters: {'n_estimators': 897, 'learning_rate': 0.29048354507676727, 'max_depth': 3, 'subsample': 0.635927791992896, 'colsample_bytree': 0.7763061041027951, 'gamma': 3.8614628927875594, 'reg_alpha': 3.961121656923509, 'reg_lambda': 1.7542715144337317}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:54,007] Trial 5 finished with value: 0.9691752141610047 and parameters: {'n_estimators': 591, 'learning_rate': 0.04002981606782065, 'max_depth': 6, 'subsample': 0.7943603602978523, 'colsample_bytree': 0.628221382748969, 'gamma': 0.26814766086619757, 'reg_alpha': 4.5808038288365625, 'reg_lambda': 0.0409043087961769}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:55,080] Trial 6 finished with value: 0.9690672519477793 and parameters: {'n_estimators': 638, 'learning_rate': 0.13052482940927174, 'max_depth': 9, 'subsample': 0.9207666006653562, 'colsample_bytree': 0.88980408732524, 'gamma': 1.2990553859887477, 'reg_alpha': 4.000991321952304, 'reg_lambda': 3.214394096804887}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:55,882] Trial 7 finished with value: 0.9690132708411667 and parameters: {'n_estimators': 933, 'learning_rate': 0.2520485269207683, 'max_depth': 4, 'subsample': 0.6404028013101546, 'colsample_bytree': 0.8454800819326219, 'gamma': 2.971050466051585, 'reg_alpha': 1.3821144118344408, 'reg_lambda': 4.981693085751223}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:58,093] Trial 8 finished with value: 0.9686353885211452 and parameters: {'n_estimators': 248, 'learning_rate': 0.013564681266237194, 'max_depth': 4, 'subsample': 0.8388578601562976, 'colsample_bytree': 0.8325058239927551, 'gamma': 3.001508651103104, 'reg_alpha': 4.678365435831949, 'reg_lambda': 1.8435946838500468}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:57:59,450] Trial 9 finished with value: 0.9691752433084705 and parameters: {'n_estimators': 688, 'learning_rate': 0.07852742236662673, 'max_depth': 8, 'subsample': 0.9000634989691813, 'colsample_bytree': 0.7863392944353069, 'gamma': 1.7012970773847869, 'reg_alpha': 0.4783321355038678, 'reg_lambda': 2.862394258712241}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:00,529] Trial 10 finished with value: 0.9690672519477795 and parameters: {'n_estimators': 427, 'learning_rate': 0.18478164720328122, 'max_depth': 10, 'subsample': 0.7182569644541837, 'colsample_bytree': 0.6492403271316217, 'gamma': 4.8066282921335475, 'reg_alpha': 2.7493669925161353, 'reg_lambda': 4.7771949965421445}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:01,675] Trial 11 finished with value: 0.9690132562674337 and parameters: {'n_estimators': 794, 'learning_rate': 0.11451759119080292, 'max_depth': 8, 'subsample': 0.9961823207901042, 'colsample_bytree': 0.6915654919894562, 'gamma': 1.816505364880688, 'reg_alpha': 0.0016537288298897335, 'reg_lambda': 3.9062993177377607}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:02,637] Trial 12 finished with value: 0.9692291952676175 and parameters: {'n_estimators': 738, 'learning_rate': 0.1831649721260718, 'max_depth': 7, 'subsample': 0.8870392258886932, 'colsample_bytree': 0.7311219367889389, 'gamma': 2.185542068308334, 'reg_alpha': 1.5226733020055658, 'reg_lambda': 3.89210722025601}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:03,717] Trial 13 finished with value: 0.9691212330543921 and parameters: {'n_estimators': 482, 'learning_rate': 0.18707308900250863, 'max_depth': 7, 'subsample': 0.8737395894793872, 'colsample_bytree': 0.710997486113011, 'gamma': 2.494405929743981, 'reg_alpha': 1.6208796466857196, 'reg_lambda': 4.0476273402393605}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:04,730] Trial 14 finished with value: 0.9692292098413503 and parameters: {'n_estimators': 773, 'learning_rate': 0.21757844069042442, 'max_depth': 7, 'subsample': 0.9595260872350212, 'colsample_bytree': 0.7274874339939429, 'gamma': 2.398091732282708, 'reg_alpha': 1.6599523924887212, 'reg_lambda': 3.880783007390546}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:05,801] Trial 15 finished with value: 0.9692292244150833 and parameters: {'n_estimators': 877, 'learning_rate': 0.2331540932639888, 'max_depth': 5, 'subsample': 0.9881912693286266, 'colsample_bytree': 0.600257622912479, 'gamma': 3.1754325088403266, 'reg_alpha': 3.207860187268011, 'reg_lambda': 4.364038582671016}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:06,614] Trial 16 finished with value: 0.9691212476281249 and parameters: {'n_estimators': 922, 'learning_rate': 0.29521369198458064, 'max_depth': 5, 'subsample': 0.7228883202427812, 'colsample_bytree': 0.6080400049834204, 'gamma': 3.648238425542407, 'reg_alpha': 3.2247651589100483, 'reg_lambda': 4.423747917755746}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:07,720] Trial 17 finished with value: 0.969013256267434 and parameters: {'n_estimators': 211, 'learning_rate': 0.25787511766557064, 'max_depth': 5, 'subsample': 0.8364941715143898, 'colsample_bytree': 0.6661784805548936, 'gamma': 4.661902433593403, 'reg_alpha': 3.355500469765984, 'reg_lambda': 3.3824133639969207}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:08,762] Trial 18 finished with value: 0.9691752433084705 and parameters: {'n_estimators': 562, 'learning_rate': 0.215691783240502, 'max_depth': 5, 'subsample': 0.6741505784823487, 'colsample_bytree': 0.6132593838373819, 'gamma': 3.1857211434440105, 'reg_alpha': 2.2056878061827527, 'reg_lambda': 4.39447742417058}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:09,777] Trial 19 finished with value: 0.9690132708411667 and parameters: {'n_estimators': 1197, 'learning_rate': 0.1524045285999078, 'max_depth': 8, 'subsample': 0.7687578590274781, 'colsample_bytree': 0.6569047941182021, 'gamma': 4.417266729091164, 'reg_alpha': 3.545647984628943, 'reg_lambda': 2.504080833034329}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:10,691] Trial 20 finished with value: 0.9690132708411667 and parameters: {'n_estimators': 840, 'learning_rate': 0.2660633089813103, 'max_depth': 10, 'subsample': 0.9476863880402552, 'colsample_bytree': 0.7536277944191386, 'gamma': 3.469293448624879, 'reg_alpha': 2.8800986446650376, 'reg_lambda': 4.365056407959595}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:11,808] Trial 21 finished with value: 0.9691212476281249 and parameters: {'n_estimators': 773, 'learning_rate': 0.21731767791846, 'max_depth': 7, 'subsample': 0.9765129626887056, 'colsample_bytree': 0.6811140428388722, 'gamma': 2.453284111286888, 'reg_alpha': 1.8692275035578092, 'reg_lambda': 3.807070092586687}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:12,679] Trial 22 finished with value: 0.9690672519477793 and parameters: {'n_estimators': 999, 'learning_rate': 0.2359857649773631, 'max_depth': 6, 'subsample': 0.9556020811751699, 'colsample_bytree': 0.7260591961111814, 'gamma': 2.8165138636417586, 'reg_alpha': 1.0316497096616999, 'reg_lambda': 3.516733445936358}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:13,586] Trial 23 finished with value: 0.9691212330543921 and parameters: {'n_estimators': 669, 'learning_rate': 0.19823428552878902, 'max_depth': 9, 'subsample': 0.9990940543200513, 'colsample_bytree': 0.6425018213766246, 'gamma': 2.0457553993359276, 'reg_alpha': 2.11027533898446, 'reg_lambda': 4.597620037281888}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:14,542] Trial 24 finished with value: 0.9692292244150833 and parameters: {'n_estimators': 525, 'learning_rate': 0.2747398233393421, 'max_depth': 8, 'subsample': 0.9413930548423871, 'colsample_bytree': 0.6003528192202207, 'gamma': 4.123453384363192, 'reg_alpha': 1.025717544779504, 'reg_lambda': 4.981382154173712}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:15,361] Trial 25 finished with value: 0.9692292244150833 and parameters: {'n_estimators': 337, 'learning_rate': 0.2741887697123679, 'max_depth': 8, 'subsample': 0.9155964593639974, 'colsample_bytree': 0.603966825087814, 'gamma': 4.186976925243854, 'reg_alpha': 3.0255975780556446, 'reg_lambda': 4.849636649991838}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:16,313] Trial 26 finished with value: 0.9690672519477793 and parameters: {'n_estimators': 526, 'learning_rate': 0.2449816630793134, 'max_depth': 9, 'subsample': 0.8678731215946435, 'colsample_bytree': 0.6835718658828998, 'gamma': 3.3393296489549393, 'reg_alpha': 1.010976410894936, 'reg_lambda': 4.994599481551471}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:17,220] Trial 27 finished with value: 0.9691752287347377 and parameters: {'n_estimators': 349, 'learning_rate': 0.27734869077990615, 'max_depth': 4, 'subsample': 0.9294134462208059, 'colsample_bytree': 0.6008939529170473, 'gamma': 4.119443423957239, 'reg_alpha': 3.7911122449670005, 'reg_lambda': 4.249189611620974}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:18,275] Trial 28 finished with value: 0.9692292098413503 and parameters: {'n_estimators': 606, 'learning_rate': 0.16181029993839308, 'max_depth': 8, 'subsample': 0.6024163812330855, 'colsample_bytree': 0.637740598889355, 'gamma': 3.650350937048231, 'reg_alpha': 1.0615311985838556, 'reg_lambda': 3.6205365109351995}. Best is trial 1 with value: 0.9692831909479629.\n[I 2025-07-14 13:58:19,246] Trial 29 finished with value: 0.9689592897345539 and parameters: {'n_estimators': 1095, 'learning_rate': 0.29605184451164634, 'max_depth': 6, 'subsample': 0.8365057519805318, 'colsample_bytree': 0.6361428647372047, 'gamma': 2.7249731431052053, 'reg_alpha': 2.4194796239502887, 'reg_lambda': 2.867191521471357}. Best is trial 1 with value: 0.9692831909479629.\n","output_type":"stream"},{"name":"stdout","text":"OOF accuracy: 0.9693370762254373\nFold accuracy: [0.9681597409606044, 0.9719373988127361, 0.9724770642201835, 0.9638424177010254, 0.9654427645788337, 0.9670626349892009, 0.9713822894168467, 0.9708423326133909, 0.9703023758099352, 0.9719222462203023]\nMean CV accuracy: 0.9693\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n# ===== OOF predictions ensemble (averaging predictions across models) =====\n# Combine out-of-fold predictions from LightGBM, CatBoost, and XGBoost by averaging\noof_preds_ensemble = (oof_preds_lgbm + oof_preds_cat + oof_preds_xgb) / 3\n\n# Convert averaged probabilities to binary predictions using a 0.5 threshold\noof_binary_ensemble = (oof_preds_ensemble > 0.5).astype(int)\n\n# Calculate overall OOF accuracy for the ensemble\noof_accuracy_ensemble = accuracy_score(y_train, oof_binary_ensemble)\nprint(\"OOF accuracy (ensemble):\", oof_accuracy_ensemble)\n\n# ===== CV accuracy per fold (based on ensemble OOF predictions) =====\n# Perform Stratified K-Fold to evaluate ensemble accuracy fold by fold\nskf_ensemble = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\ncv_scores_ensemble = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf_ensemble.split(X_train, y_train)):\n    # Average the OOF predictions for the current fold from all models\n    oof_fold = (oof_preds_lgbm[val_idx] + oof_preds_cat[val_idx] + oof_preds_xgb[val_idx]) / 3\n    \n    # Convert probabilities to binary predictions\n    oof_fold_binary = (oof_fold > 0.5).astype(int)\n    \n    # Calculate accuracy for the current fold\n    acc = accuracy_score(np.array(y_train)[val_idx], oof_fold_binary)\n    cv_scores_ensemble.append(acc)\n\n# Calculate mean cross-validation accuracy across all folds\nmean_cv_accuracy_ensemble = np.mean(cv_scores_ensemble)\nprint(f\"Fold accuracy (ensemble): {cv_scores_ensemble}\")\nprint(f\"Mean CV accuracy (ensemble): {mean_cv_accuracy_ensemble:.4f}\")\n\n# ===== Test predictions ensemble (average predictions across models and folds) =====\n# Average test predictions over folds for each model\nmean_preds_lgbm = test_preds_lgbm.mean(axis=1)\nmean_preds_cat = test_preds_cat.mean(axis=1)\nmean_preds_xgb = test_preds_xgb.mean(axis=1)\n\n# Combine predictions from all three models by averaging\nensemble_test = (mean_preds_lgbm + mean_preds_cat + mean_preds_xgb) / 3\n\n# Final binary predictions on the test set using a 0.5 threshold\nfinal_ensemble_pred = (ensemble_test > 0.5).astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:58:27.208729Z","iopub.execute_input":"2025-07-14T13:58:27.209046Z","iopub.status.idle":"2025-07-14T13:58:27.235183Z","shell.execute_reply.started":"2025-07-14T13:58:27.209021Z","shell.execute_reply":"2025-07-14T13:58:27.234375Z"}},"outputs":[{"name":"stdout","text":"OOF accuracy (ensemble): 0.9692291081839776\nFold accuracy (ensemble): [0.9681597409606044, 0.9719373988127361, 0.9724770642201835, 0.963302752293578, 0.9654427645788337, 0.9670626349892009, 0.9713822894168467, 0.9708423326133909, 0.9703023758099352, 0.9713822894168467]\nMean CV accuracy (ensemble): 0.9692\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Convert binary predictions (0,1) to string labels ('Extrovert', 'Introvert')\nfinal_ensemble_pred_labels = pd.Series(final_ensemble_pred).map({0: 'Extrovert', 1: 'Introvert'}).values\n\n# Load the sample submission file which contains the required format and IDs\nssub = pd.read_csv(\"/kaggle/input/playground-series-s5e7/sample_submission.csv\")\n\n# Replace the 'Personality' column with our predicted personality labels\nssub['Personality'] = final_ensemble_pred_labels\n\n# Save the updated DataFrame to a CSV file without including the index\nssub.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:58:42.190089Z","iopub.execute_input":"2025-07-14T13:58:42.190621Z","iopub.status.idle":"2025-07-14T13:58:42.211487Z","shell.execute_reply.started":"2025-07-14T13:58:42.190593Z","shell.execute_reply":"2025-07-14T13:58:42.210436Z"}},"outputs":[],"execution_count":12}]}